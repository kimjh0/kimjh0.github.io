---
title: "NoSQL 을 사용한 카카오톡 메시지 분석"
tags:
  - Python
---

## 개요
카카오톡 대화내용을 분석하여 순위를 매김

통계는 MongoDB 를 사용

## 목표
- 날짜별 대화 개수 순위
- 시간별 대화 개수 순위
- 사람별 대화 개수 순위
- 이모티콘 개수 순위
- 사진 개수 순위
- 단어 빈도 순위

## 개발
### 환경
- Linux 서버 : MongoDB
- Windows 클라이언트 : 파이썬

### 서버측 MongoDB 설치
- docker 사용

```bash
$ docker run --name mongo -p 27017:27017 -d mongo
```

외부 접속을 허용하기 위해 port 연결

### 클라이언트측 Python 설치
#### konlpy 패키지 설치
단어 파싱을 위해 konlpy 패키지 설치

konlpy 는 JDK 가 필요하고, 윈도우에서 빌드된 OpenJDK 가 64bit 라 Python 도 맞춤

bit 가 맞지 않으면 jvm.dll 로드에 실패함

- OpenJDK 64 bit 설치
- python 64 bit 설치

pip 를 통해 konlpy 를 설치하면 알맞은 bit 의 패키지가 설치됨

의존성에 의해 JPype1 도 같이 설치됨

```bash
$ python -m pip install --upgrade pip
$ python -m pip install konlpy
```

#### pymongo 패키지 설치
MongoDB 를 사용하기 위해 pymongo 패키지 설치

```bash
$ python -m pip install pymongo
```

### 구현
- 카카오톡 앱에서 내보내기된 대화 내역을 Python 으로 파싱하여 DB 에 입력
- DB 에서 적절한 쿼리를 통해 통계 계산

#### 코드

```python
import os  
from datetime import datetime  
from konlpy.tag import Okt  
import pymongo  
  
okt = Okt()  
db_conn = None  
  
  
def conn_db(address, database, collection):  
    connection = pymongo.MongoClient(address)  
    db = connection[database]  
    coll = db[collection]  
    coll.delete_many({})  
    return coll  
  
  
def get_datetime(msg):  
    try:  
        msg = msg.strip()  
        idx = msg.index('일')  
        sdate = msg[:idx + 1].strip()  
        stime = msg[idx + 1:].strip()  
        if stime[0:2] == '오전':  
            return datetime.strptime(sdate + ' AM ' + stime[3:], '%Y년 %m월 %d일 %p %I:%M')  
        if stime[0:2] == '오후':  
            return datetime.strptime(sdate + ' PM ' + stime[3:], '%Y년 %m월 %d일 %p %I:%M')  
  
        return None  
  
 except ValueError:  
        return None  
  
  
def make_bulk_data(bulk_data, date, name, msg):  
    ret = okt.pos(msg)  
  
    data = {  
        'date': date,  
  'name': name,  
  'msg': msg,  
  }  
  
    for i in ret:  
        if i[1] == 'Foreign':  
            continue  
 elif i[1] not in data:  
            data.update({  
                i[1]: [i[0]]  
            })  
        else:  
            data[i[1]].append(i[0])  
  
    bulk_data.append(data)  
  
  
def parse_talk(filepath, db_conn):  
    f = open(filepath, 'r', encoding='utf-8')  
  
    f.seek(0, os.SEEK_END)  
    size = f.tell()  
    f.seek(0, os.SEEK_SET)  
  
    title = f.readline().strip()  
    export_date = f.readline().strip()  
    export_date = get_datetime(export_date[export_date.index(':') + 1:])  
    start_date = None  
  end_date = None  
  
  date = None  
  name = None  
  msg = None  
  bulk_data = list()  
  
    progress = 0  
  while True:  
        line = f.readline()  
        if not line:  
            break  
 if line in f.newlines:  
            continue  
  
  curr = int(f.tell() / size * 100)  
        if curr != progress and curr <= 100:  
            print('%d %%' % curr)  
            progress = curr  
  
        if len(bulk_data) > 1000:  
            db_conn.insert_many(bulk_data)  
            bulk_data.clear()  
  
        try:  
            idx = line.index(',')  
            date = get_datetime(line[:idx])  
            if date is None:  
                raise ValueError  
  
  if start_date is None:  
                start_date = date  
            end_date = date  
  
            line = line[idx + 1:].strip()  
            idx = line.index(':')  
            name = line[:idx].strip()  
            msg = line[idx + 1:].strip()  
  
            make_bulk_data(bulk_data, date, name, msg)  
  
        except ValueError:  
            if date is None:  
                continue  
 if name is None:  
                continue  
  
  line = line.strip()  
            if len(line) < 1:  
                continue  
  
 if get_datetime(line) is not None:  
                continue  
  
  make_bulk_data(bulk_data, date, name, line)  
            continue  
  
  f.close()  
  
    db_conn.insert_many(bulk_data)  
  
    print('title', title)  
    print('start_date', start_date)  
    print('end_date', end_date)  
    print('export_date', export_date)  
  
  
db_conn = conn_db('mongodb://192.168.50.152:27017', 'test', 'talk')  
parse_talk('talk.txt', db_conn)  
  
## msg count by date  
print('날짜 순위')  
pipeline = [  
    {  
        '$group': {  
            '_id': {  
                '$dateToString': {  
                    'format': '%Y-%m-%d',  
  'date': '$date'  
  }  
            },  
  'count': {'$sum': 1}  
        }  
    },  
  {  
        '$sort': {'count': -1}  
    },  
  {  
        '$limit': 10  
  }  
]  
for i in db_conn.aggregate(pipeline):  
    print(i)  
  
## msg count by hour  
print('시간 순위')  
pipeline = [  
    {  
        '$group': {  
            '_id': {'$hour': '$date'},  
  'count': {'$sum': 1}  
        }  
    },  
  {  
        '$sort': {'count': -1}  
    }  
]  
for i in db_conn.aggregate(pipeline):  
    print(i)  
  
## msg count by name  
print('메시지 순위')  
pipeline = [  
    {  
        '$group': {  
            '_id': '$name',  
  'count': {'$sum': 1}  
        }  
    },  
  {  
        '$sort': {'count': -1}  
    }  
]  
for i in db_conn.aggregate(pipeline):  
    print(i)  
  
## emoji count by name  
print('이모티콘 순위')  
pipeline = [  
    {  
        '$match': {'msg': '이모티콘'}  
    },  
  {  
        '$group': {  
            '_id': '$name',  
  'count': {'$sum': 1}  
        }  
    },  
  {  
        '$sort': {'count': -1}  
    }  
]  
for i in db_conn.aggregate(pipeline):  
    print(i)  
  
## pic count by name  
print('사진 순위')  
pipeline = [  
    {  
        '$match': {'msg': '사진'}  
    },  
  {  
        '$group': {  
            '_id': '$name',  
  'count': {'$sum': 1}  
        }  
    },  
  {  
        '$sort': {'count': -1}  
    }  
]  
for i in db_conn.aggregate(pipeline):  
    print(i)  
  
## word top  
print('단어 순위')  
pipeline = [  
    {  
        '$project': {  
            '_id': False,  
  'tags': {  
                '$concatArrays': [  
                    {'$ifNull': ['$Noun', []]},  
  # {'$ifNull': ['$KoreanParticle', []]},  
 # {'$ifNull': ['$Adjective', []]}, # {'$ifNull': [ '$Josa', [] ]}, # {'$ifNull': [ '$Verb', [] ]}, # {'$ifNull': ['$Punctuation', []]}, # {'$ifNull': [ '$Conjunction', [] ]}, # {'$ifNull': [ '$Suffix', [] ]}, # {'$ifNull': [ '$Adverb', [] ]}, # {'$ifNull': [ '$VerbPrefix', [] ]}, # {'$ifNull': [ '$Adjective', [] ]}, # {'$ifNull': [ '$Determiner', [] ]}, # {'$ifNull': [ '$Modifier', [] ]}, # {'$ifNull': [ '$Number', [] ]},  ]  
            }  
        }  
    },  
  {  
        '$unwind': {'path': '$tags'}  
    },  
  {  
        '$group': {  
            '_id': '$tags',  
  'count': {'$sum': 1.0}  
        }  
    },  
  {  
        '$sort': {'count': -1}  
    },  
  {  
        '$limit': 30  
  }  
]  
for i in db_conn.aggregate(pipeline):  
    print(i)
```
